{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Accelerating_faster_rcnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8piyLn3KCPYV"
      },
      "source": [
        "# !wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoRTyUc94S1a"
      },
      "source": [
        "# Two-Stage Object Detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiCxeTJNdIA_"
      },
      "source": [
        "!pip install git+https://github.com/deepvision-class/starter-code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WOlQ6aJyFDQ",
        "outputId": "b73f6aa8-9ffd-412b-f02d-bb4b16befd05"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import coutils\n",
        "from coutils import extract_drive_file_id, register_colab_notebooks, \\\n",
        "                    fix_random_seed, rel_error\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import copy\n",
        "import time\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# for plotting\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# data type and device for torch.tensor\n",
        "to_float = {'dtype': torch.float, 'device': 'cpu'}\n",
        "to_float_cuda = {'dtype': torch.float, 'device': 'cuda'}\n",
        "to_double = {'dtype': torch.double, 'device': 'cpu'}\n",
        "to_double_cuda = {'dtype': torch.double, 'device': 'cuda'}\n",
        "to_long = {'dtype': torch.long, 'device': 'cpu'}\n",
        "to_long_cuda = {'dtype': torch.long, 'device': 'cuda'}\n",
        "\n",
        "# for mAP evaluation\n",
        "!rm -rf mAP\n",
        "!git clone https://github.com/Cartucho/mAP.git\n",
        "!rm -rf mAP/input/*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mAP'...\n",
            "remote: Enumerating objects: 908, done.\u001b[K\n",
            "remote: Total 908 (delta 0), reused 0 (delta 0), pack-reused 908\u001b[K\n",
            "Receiving objects: 100% (908/908), 14.71 MiB | 10.58 MiB/s, done.\n",
            "Resolving deltas: 100% (321/321), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVbv-6c14M54"
      },
      "source": [
        "def data_visualizer(img, idx_to_class, bbox=None, pred=None):\n",
        "  img_copy = np.array(img).astype('uint8')\n",
        "\n",
        "  if bbox is not None:\n",
        "    for bbox_idx in range(bbox.shape[0]):\n",
        "      one_bbox = bbox[bbox_idx][:4]\n",
        "      cv2.rectangle(img_copy, (one_bbox[0], one_bbox[1]), (one_bbox[2],\n",
        "                  one_bbox[3]), (255, 0, 0), 2)\n",
        "      if bbox.shape[1] > 4: # if class info provided\n",
        "        obj_cls = idx_to_class[bbox[bbox_idx][4].item()]\n",
        "        cv2.putText(img_copy, '%s' % (obj_cls),\n",
        "                  (one_bbox[0], one_bbox[1]+15),\n",
        "                  cv2.FONT_HERSHEY_PLAIN, 1.0, (0, 0, 255), thickness=1)\n",
        "\n",
        "  if pred is not None:\n",
        "    for bbox_idx in range(pred.shape[0]):\n",
        "      one_bbox = pred[bbox_idx][:4]\n",
        "      cv2.rectangle(img_copy, (one_bbox[0], one_bbox[1]), (one_bbox[2],\n",
        "                  one_bbox[3]), (0, 255, 0), 2)\n",
        "      \n",
        "      if pred.shape[1] > 4: # if class and conf score info provided\n",
        "        obj_cls = idx_to_class[pred[bbox_idx][4].item()]\n",
        "        conf_score = pred[bbox_idx][5].item()\n",
        "        cv2.putText(img_copy, '%s, %.2f' % (obj_cls, conf_score),\n",
        "                    (one_bbox[0], one_bbox[1]+15),\n",
        "                    cv2.FONT_HERSHEY_PLAIN, 1.0, (0, 0, 255), thickness=1)\n",
        "\n",
        "  plt.imshow(img_copy)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpThJc1jyVlD"
      },
      "source": [
        "def get_pascal_voc2007_data(image_root, split='train'):\n",
        "  from torchvision import datasets\n",
        "\n",
        "  train_dataset = datasets.VOCDetection(image_root, year='2007', image_set=split,\n",
        "                                    download=False)\n",
        "  \n",
        "  return train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XyRnMpWynaa"
      },
      "source": [
        "def pascal_voc2007_loader(dataset, batch_size, num_workers=0):\n",
        "  \"\"\"\n",
        "  Data loader for Pascal VOC 2007.\n",
        "  https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "  \"\"\"\n",
        "  from torch.utils.data import DataLoader\n",
        "  # turn off shuffle so we can index the original image\n",
        "  train_loader = DataLoader(dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=True, pin_memory=True,\n",
        "                            num_workers=num_workers,\n",
        "                            collate_fn=voc_collate_fn)\n",
        "  return train_loader\n",
        "\n",
        "\n",
        "class_to_idx = {'aeroplane':0, 'bicycle':1, 'bird':2, 'boat':3, 'bottle':4,\n",
        "                'bus':5, 'car':6, 'cat':7, 'chair':8, 'cow':9, 'diningtable':10,\n",
        "                'dog':11, 'horse':12, 'motorbike':13, 'person':14, 'pottedplant':15,\n",
        "                'sheep':16, 'sofa':17, 'train':18, 'tvmonitor':19\n",
        "}\n",
        "idx_to_class = {i:c for c, i in class_to_idx.items()}\n",
        "\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "def voc_collate_fn(batch_lst, reshape_size=224):\n",
        "    preprocess = transforms.Compose([\n",
        "      transforms.Resize((reshape_size, reshape_size)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "      ])\n",
        "    \n",
        "    batch_size = len(batch_lst)\n",
        "    \n",
        "    img_batch = torch.zeros(batch_size, 3, reshape_size, reshape_size)\n",
        "    \n",
        "    max_num_box = max(len(batch_lst[i][1]['annotation']['object']) \\\n",
        "                      for i in range(batch_size))\n",
        "\n",
        "    box_batch = torch.Tensor(batch_size, max_num_box, 5).fill_(-1.)\n",
        "    w_list = []\n",
        "    h_list = []\n",
        "    img_id_list = []\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "      img, ann = batch_lst[i]\n",
        "      w_list.append(img.size[0]) # image width\n",
        "      h_list.append(img.size[1]) # image height\n",
        "      img_id_list.append(ann['annotation']['filename'])\n",
        "      img_batch[i] = preprocess(img)\n",
        "      all_bbox = ann['annotation']['object']\n",
        "      if type(all_bbox) == dict: # inconsistency in the annotation file\n",
        "        all_bbox = [all_bbox]\n",
        "      for bbox_idx, one_bbox in enumerate(all_bbox):\n",
        "        bbox = one_bbox['bndbox']\n",
        "        obj_cls = one_bbox['name']\n",
        "        box_batch[i][bbox_idx] = torch.Tensor([float(bbox['xmin']), float(bbox['ymin']),\n",
        "          float(bbox['xmax']), float(bbox['ymax']), class_to_idx[obj_cls]])\n",
        "    \n",
        "    h_batch = torch.tensor(h_list)\n",
        "    w_batch = torch.tensor(w_list)\n",
        "\n",
        "    return img_batch, box_batch, w_batch, h_batch, img_id_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjJ3uyYBg3Lw"
      },
      "source": [
        "## Load PASCAL VOC 2007 data\n",
        "As in the previous notebook, we will use the PASCAL VOC 2007 dataset to train our object detection system.\n",
        "\n",
        "As in the previous notebook, we will subsample the dataset and wrap it in a DataLoader that can form minibatches for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRqkF5YGys-I"
      },
      "source": [
        "train_dataset = get_pascal_voc2007_data('/content', 'train')\n",
        "val_dataset = get_pascal_voc2007_data('/content', 'val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w9fLdxbywSg"
      },
      "source": [
        "class_to_idx = {'aeroplane':0, 'bicycle':1, 'bird':2, 'boat':3, 'bottle':4,\n",
        "                'bus':5, 'car':6, 'cat':7, 'chair':8, 'cow':9, 'diningtable':10,\n",
        "                'dog':11, 'horse':12, 'motorbike':13, 'person':14, 'pottedplant':15,\n",
        "                'sheep':16, 'sofa':17, 'train':18, 'tvmonitor':19\n",
        "}\n",
        "idx_to_class = {i:c for c, i in class_to_idx.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlKqy0RjyzNw"
      },
      "source": [
        "train_dataset = torch.utils.data.Subset(train_dataset, torch.arange(0, 2500)) # use 2500 samples for training\n",
        "train_loader = pascal_voc2007_loader(train_dataset, 10)\n",
        "val_loader = pascal_voc2007_loader(val_dataset, 10)\n",
        "train_loader_iter = iter(train_loader)\n",
        "img, ann, _, _, _ = train_loader_iter.next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNi5Qm6k0TVE"
      },
      "source": [
        "def coord_trans(bbox, w_pixel, h_pixel, w_amap=7, h_amap=7, mode='a2p'):\n",
        "  assert mode in ('p2a', 'a2p'), 'invalid coordinate transformation mode!'\n",
        "  assert bbox.shape[-1] >= 4, 'the transformation is applied to the first 4 values of dim -1'\n",
        "  \n",
        "  if bbox.shape[0] == 0: # corner cases\n",
        "    return bbox\n",
        "\n",
        "  resized_bbox = bbox.clone()\n",
        "  # could still work if the first dim of bbox is not batch size\n",
        "  # in that case, w_pixel and h_pixel will be scalars\n",
        "  resized_bbox = resized_bbox.view(bbox.shape[0], -1, bbox.shape[-1])\n",
        "  invalid_bbox_mask = (resized_bbox == -1) # indicating invalid bbox\n",
        "\n",
        "  if mode == 'p2a':\n",
        "    # pixel to activation\n",
        "    width_ratio = w_pixel * 1. / w_amap\n",
        "    height_ratio = h_pixel * 1. / h_amap\n",
        "    resized_bbox[:, :, [0, 2]] /= width_ratio.view(-1, 1, 1)\n",
        "    resized_bbox[:, :, [1, 3]] /= height_ratio.view(-1, 1, 1)\n",
        "  else:\n",
        "    # activation to pixel\n",
        "    width_ratio = w_pixel * 1. / w_amap\n",
        "    height_ratio = h_pixel * 1. / h_amap\n",
        "    resized_bbox[:, :, [0, 2]] *= width_ratio.view(-1, 1, 1)\n",
        "    resized_bbox[:, :, [1, 3]] *= height_ratio.view(-1, 1, 1)\n",
        "\n",
        "  resized_bbox.masked_fill_(invalid_bbox_mask, -1)\n",
        "  resized_bbox.resize_as_(bbox)\n",
        "  return resized_bbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woGSE8EOfhIT"
      },
      "source": [
        "anchor_list = torch.tensor([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [2, 3], [3, 2], [3, 5], [5, 3]], **to_float_cuda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAXdfCYFy8PG"
      },
      "source": [
        "def GenerateGrid(batch_size, w_amap=7, h_amap=7, dtype=torch.float32, device='cuda'):\n",
        "  w_range = torch.arange(0, w_amap, dtype=dtype, device=device) + 0.5\n",
        "  h_range = torch.arange(0, h_amap, dtype=dtype, device=device) + 0.5\n",
        "\n",
        "  w_grid_idx = w_range.unsqueeze(0).repeat(h_amap, 1)\n",
        "  h_grid_idx = h_range.unsqueeze(1).repeat(1, w_amap)\n",
        "  grid = torch.stack([w_grid_idx, h_grid_idx], dim=-1)\n",
        "  grid = grid.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n",
        "\n",
        "  return grid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Pihi_ghy-px"
      },
      "source": [
        "def GenerateAnchor(anc, grid):\n",
        "  B, H_amap, W_amap, _ = grid.shape\n",
        "  A, _ = anc.shape\n",
        "  anchors = torch.zeros(B, A, H_amap, W_amap, 4).to(grid)\n",
        "  for a in range(A):\n",
        "    anchors[:, a, :, :, 0] = grid[:, :, :, 0] - anc[a, 0] / 2.0\n",
        "    anchors[:, a, :, :, 1] = grid[:, :, :, 1] - anc[a, 1] / 2.0\n",
        "    anchors[:, a, :, :, 2] = grid[:, :, :, 0] + anc[a, 0] / 2.0\n",
        "    anchors[:, a, :, :, 3] = grid[:, :, :, 1] + anc[a, 1] / 2.0\n",
        "  return anchors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GeT6JZI4Gjo"
      },
      "source": [
        "def GenerateProposal(anchors, offsets, method='YOLO'):\n",
        "  assert(method in ['YOLO', 'FasterRCNN'])\n",
        "  proposals = None\n",
        "  B, A, H, W, _ = anchors.shape\n",
        "  proposals = torch.zeros_like(anchors)\n",
        "  proposals_transfer = torch.zeros_like(anchors)\n",
        "  anchors_transfer = torch.zeros_like(anchors)\n",
        "  anchors_transfer[:, :, :, :, 0] = (anchors[:, :, :, :, 0] + anchors[:, :, :, :, 2]) / 2\n",
        "  anchors_transfer[:, :, :, :, 1] = (anchors[:, :, :, :, 1] + anchors[:, :, :, :, 3]) / 2\n",
        "  anchors_transfer[:, :, :, :, 2] = anchors[:, :, :, :, 2] - anchors[:, :, :, :, 0]\n",
        "  anchors_transfer[:, :, :, :, 3] = anchors[:, :, :, :, 3] - anchors[:, :, :, :, 1]\n",
        "  if method == 'YOLO':\n",
        "    proposals_transfer[:, :, :, :, 0] = anchors_transfer[:, :, :, :, 0] + offsets[:, :, :, :, 0]\n",
        "    proposals_transfer[:, :, :, :, 1] = anchors_transfer[:, :, :, :, 1] + offsets[:, :, :, :, 1]\n",
        "    proposals_transfer[:, :, :, :, 2] = anchors_transfer[:, :, :, :, 2] * torch.exp(offsets[:, :, :, :, 2])\n",
        "    proposals_transfer[:, :, :, :, 3] = anchors_transfer[:, :, :, :, 3] * torch.exp(offsets[:, :, :, :, 3])\n",
        "  else:\n",
        "    proposals_transfer[:, :, :, :, 0] = anchors_transfer[:, :, :, :, 0] + offsets[:, :, :, :, 0] * anchors_transfer[:, :, :, :, 2]\n",
        "    proposals_transfer[:, :, :, :, 1] = anchors_transfer[:, :, :, :, 1] + offsets[:, :, :, :, 1] * anchors_transfer[:, :, :, :, 3]\n",
        "    proposals_transfer[:, :, :, :, 2] = anchors_transfer[:, :, :, :, 2] * torch.exp(offsets[:, :, :, :, 2])\n",
        "    proposals_transfer[:, :, :, :, 3] = anchors_transfer[:, :, :, :, 3] * torch.exp(offsets[:, :, :, :, 3])\n",
        "  proposals[:, :, :, :, 0] = proposals_transfer[:, :, :, :, 0] - proposals_transfer[:, :, :, :, 2] / 2\n",
        "  proposals[:, :, :, :, 1] = proposals_transfer[:, :, :, :, 1] - proposals_transfer[:, :, :, :, 3] / 2\n",
        "  proposals[:, :, :, :, 2] = proposals_transfer[:, :, :, :, 0] + proposals_transfer[:, :, :, :, 2] / 2\n",
        "  proposals[:, :, :, :, 3] = proposals_transfer[:, :, :, :, 1] + proposals_transfer[:, :, :, :, 3] / 2\n",
        "  return proposals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOy6S05nzClZ"
      },
      "source": [
        "def IoU(proposals, bboxes):\n",
        "  iou_mat = None\n",
        "  B, A, H, W, _ = proposals.shape\n",
        "  B, N, _ = bboxes.shape\n",
        "  proposals = proposals.reshape(B, A * H * W, 4).repeat(1, 1, N).reshape(B, A * H * W, N, 4)\n",
        "  bboxes = bboxes.repeat(1, A * H * W, 1).reshape(B, A * H * W, N, 5)\n",
        "  xa = torch.max(proposals[:, :, :, 0], bboxes[:, :, :, 0])\n",
        "  ya = torch.max(proposals[:, :, :, 1], bboxes[:, :, :, 1])\n",
        "  xb = torch.min(proposals[:, :, :, 2], bboxes[:, :, :, 2])\n",
        "  yb = torch.min(proposals[:, :, :, 3], bboxes[:, :, :, 3])\n",
        "  zero = torch.zeros_like(xa)\n",
        "  intersection = torch.max(zero, (xb - xa)) * torch.max(zero, (yb - ya))\n",
        "  bbox_area = (bboxes[:, :, :, 2] - bboxes[:, :, :, 0]) * (bboxes[:, :, :, 3] - bboxes[:, :, :, 1])\n",
        "  proposal_area = (proposals[:, :, :, 2] - proposals[:, :, :, 0]) * (proposals[:, :, :, 3] - proposals[:, :, :, 1])\n",
        "  union = bbox_area + proposal_area - intersection\n",
        "  iou_mat = intersection / union\n",
        "  return iou_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y42FIPn88c7N"
      },
      "source": [
        "def ReferenceOnActivatedAnchors(anchors, bboxes, grid, iou_mat, pos_thresh=0.7, neg_thresh=0.3, method='FasterRCNN'):\n",
        "  assert(method in ['FasterRCNN', 'YOLO'])\n",
        "\n",
        "  B, A, h_amap, w_amap, _ = anchors.shape\n",
        "  N = bboxes.shape[1]\n",
        "\n",
        "  # activated/positive anchors\n",
        "  max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "  if method == 'FasterRCNN':\n",
        "    max_iou_per_box = iou_mat.max(dim=1, keepdim=True)[0]\n",
        "    activated_anc_mask = (iou_mat == max_iou_per_box) & (max_iou_per_box > 0)\n",
        "    activated_anc_mask |= (iou_mat > pos_thresh) # using the pos_thresh condition as well\n",
        "    # if an anchor matches multiple GT boxes, choose the box with the largest iou\n",
        "    activated_anc_mask = activated_anc_mask.max(dim=-1)[0] # Bx(AxH’xW’)\n",
        "    activated_anc_ind = torch.nonzero(activated_anc_mask.view(-1)).squeeze(-1)\n",
        "\n",
        "    # GT conf scores\n",
        "    GT_conf_scores = max_iou_per_anc[activated_anc_mask] # M\n",
        "\n",
        "    # GT class\n",
        "    box_cls = bboxes[:, :, 4].view(B, 1, N).expand((B, A*h_amap*w_amap, N))\n",
        "    GT_class = torch.gather(box_cls, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1) # M\n",
        "    GT_class = GT_class[activated_anc_mask].long()\n",
        "\n",
        "    bboxes_expand = bboxes[:, :, :4].view(B, 1, N, 4).expand((B, A*h_amap*w_amap, N, 4))\n",
        "    bboxes = torch.gather(bboxes_expand, -2, max_iou_per_anc_ind.unsqueeze(-1) \\\n",
        "      .unsqueeze(-1).expand(B, A*h_amap*w_amap, 1, 4)).view(-1, 4)\n",
        "    bboxes = bboxes[activated_anc_ind]\n",
        "  else:\n",
        "    bbox_mask = (bboxes[:, :, 0] != -1) # BxN, indicate invalid boxes\n",
        "    bbox_centers = (bboxes[:, :, 2:4] - bboxes[:, :, :2]) / 2. + bboxes[:, :, :2] # BxNx2\n",
        "\n",
        "    mah_dist = torch.abs(grid.view(B, -1, 2).unsqueeze(2) - bbox_centers.unsqueeze(1)).sum(dim=-1) # Bx(H'xW')xN\n",
        "    min_mah_dist = mah_dist.min(dim=1, keepdim=True)[0] # Bx1xN\n",
        "    grid_mask = (mah_dist == min_mah_dist).unsqueeze(1) # Bx1x(H'xW')xN\n",
        "\n",
        "    reshaped_iou_mat = iou_mat.view(B, A, -1, N)\n",
        "    anc_with_largest_iou = reshaped_iou_mat.max(dim=1, keepdim=True)[0] # Bx1x(H’xW’)xN\n",
        "    anc_mask = (anc_with_largest_iou == reshaped_iou_mat) # BxAx(H’xW’)xN\n",
        "    activated_anc_mask = (grid_mask & anc_mask).view(B, -1, N)\n",
        "    activated_anc_mask &= bbox_mask.unsqueeze(1)\n",
        "    \n",
        "    # one anchor could match multiple GT boxes\n",
        "    activated_anc_ind = torch.nonzero(activated_anc_mask.view(-1)).squeeze(-1)\n",
        "    GT_conf_scores = iou_mat.view(-1)[activated_anc_ind]\n",
        "    bboxes = bboxes.view(B, 1, N, 5).repeat(1, A*h_amap*w_amap, 1, 1).view(-1, 5)[activated_anc_ind]\n",
        "    GT_class = bboxes[:, 4].long()\n",
        "    bboxes = bboxes[:, :4]\n",
        "    activated_anc_ind = (activated_anc_ind / activated_anc_mask.shape[-1]).long()\n",
        "\n",
        "  print('number of pos proposals: ', activated_anc_ind.shape[0])\n",
        "  activated_anc_coord = anchors.view(-1, 4)[activated_anc_ind]\n",
        "\n",
        "  # GT offsets\n",
        "  # bbox and anchor coordinates are x_tl, y_tl, x_br, y_br\n",
        "  # offsets are t_x, t_y, t_w, t_h\n",
        "  wh_offsets = torch.log((bboxes[:, 2:4] - bboxes[:, :2]) \\\n",
        "    / (activated_anc_coord[:, 2:4] - activated_anc_coord[:, :2]))\n",
        "\n",
        "  xy_offsets = (bboxes[:, :2] + bboxes[:, 2:4] - \\\n",
        "    activated_anc_coord[:, :2] - activated_anc_coord[:, 2:4]) / 2.\n",
        "\n",
        "  if method == \"FasterRCNN\":\n",
        "    xy_offsets /= (activated_anc_coord[:, 2:4] - activated_anc_coord[:, :2])\n",
        "  else:\n",
        "    assert torch.max(torch.abs(xy_offsets)) <= 0.5, \\\n",
        "      \"x and y offsets should be between -0.5 and 0.5! Got {}\".format( \\\n",
        "      torch.max(torch.abs(xy_offsets)))\n",
        "\n",
        "  GT_offsets = torch.cat((xy_offsets, wh_offsets), dim=-1)\n",
        "\n",
        "  # negative anchors\n",
        "  negative_anc_mask = (max_iou_per_anc < neg_thresh) # Bx(AxH’xW’)\n",
        "  negative_anc_ind = torch.nonzero(negative_anc_mask.view(-1)).squeeze(-1)\n",
        "  negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (activated_anc_ind.shape[0],))]\n",
        "  negative_anc_coord = anchors.view(-1, 4)[negative_anc_ind.view(-1)]\n",
        "  \n",
        "  # activated_anc_coord and negative_anc_coord are mainly for visualization purposes\n",
        "  return activated_anc_ind, negative_anc_ind, GT_conf_scores, GT_offsets, GT_class, \\\n",
        "         activated_anc_coord, negative_anc_coord"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_z3fY560DIv",
        "outputId": "c2b90b61-033f-4e46-bf52-f393040d477f"
      },
      "source": [
        "# default examples for visualization\n",
        "fix_random_seed(0)\n",
        "batch_size = 3\n",
        "sampled_idx = torch.linspace(0, len(train_dataset)-1, steps=batch_size).long()\n",
        "\n",
        "# get the size of each image first\n",
        "h_list = []\n",
        "w_list = []\n",
        "img_list = [] # list of images\n",
        "MAX_NUM_BBOX = 40\n",
        "box_list = torch.LongTensor(batch_size, MAX_NUM_BBOX, 5).fill_(-1) # PADDED GT boxes\n",
        "\n",
        "for idx, i in enumerate(sampled_idx):\n",
        "  # hack to get the original image so we don't have to load from local again...\n",
        "  img, ann = train_dataset.__getitem__(i)\n",
        "  img_list.append(img)\n",
        "\n",
        "  all_bbox = ann['annotation']['object']\n",
        "  if type(all_bbox) == dict:\n",
        "    all_bbox = [all_bbox]\n",
        "  for bbox_idx, one_bbox in enumerate(all_bbox):\n",
        "    bbox = one_bbox['bndbox']\n",
        "    obj_cls = one_bbox['name']\n",
        "    box_list[idx][bbox_idx] = torch.LongTensor([int(bbox['xmin']), int(bbox['ymin']),\n",
        "      int(bbox['xmax']), int(bbox['ymax']), class_to_idx[obj_cls]])\n",
        "\n",
        "  # get sizes\n",
        "  img = np.array(img)\n",
        "  w_list.append(img.shape[1])\n",
        "  h_list.append(img.shape[0])\n",
        "\n",
        "w_list = torch.tensor(w_list, **to_float_cuda)\n",
        "h_list = torch.tensor(h_list, **to_float_cuda)\n",
        "box_list = torch.tensor(box_list, **to_float_cuda)\n",
        "resized_box_list = coord_trans(box_list, w_list, h_list, mode='p2a')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NlCWtOfzLXP"
      },
      "source": [
        "class ProposalModule(nn.Module):\n",
        "  def __init__(self, in_dim, hidden_dim=256, num_anchors=9, drop_ratio=0.3):\n",
        "    super().__init__()\n",
        "\n",
        "    assert(num_anchors != 0)\n",
        "    self.num_anchors = num_anchors\n",
        "    self.base = nn.Sequential(\n",
        "        nn.Conv2d(in_dim, hidden_dim, 3, 1, 1),\n",
        "        nn.Dropout(p=drop_ratio),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Conv2d(hidden_dim, 6 * self.num_anchors, 1, 1, 0)\n",
        "    )\n",
        "\n",
        "  def _extract_anchor_data(self, anchor_data, anchor_idx):\n",
        "    B, A, D, H, W = anchor_data.shape\n",
        "    anchor_data = anchor_data.permute(0, 1, 3, 4, 2).contiguous().view(-1, D)\n",
        "    extracted_anchors = anchor_data[anchor_idx]\n",
        "    return extracted_anchors\n",
        "\n",
        "  def forward(self, features, pos_anchor_coord=None, \\\n",
        "              pos_anchor_idx=None, neg_anchor_idx=None):\n",
        "    if pos_anchor_coord is None or pos_anchor_idx is None or neg_anchor_idx is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "    conf_scores, offsets, proposals = None, None, None\n",
        "    predictions = self.base(features)\n",
        "    B, _, H, W = predictions.shape\n",
        "    conf_scores = predictions.view(B, self.num_anchors, 6, H, W)[:, :, :2, :, :]\n",
        "    offsets = predictions.view(B, self.num_anchors, 6, H, W)[:, :, 2:, :, :]\n",
        "    if mode == 'train':\n",
        "      anchor_idx = torch.cat((pos_anchor_idx, neg_anchor_idx), 0)\n",
        "      conf_scores = self._extract_anchor_data(conf_scores, anchor_idx)\n",
        "      offsets = self._extract_anchor_data(offsets, pos_anchor_idx)\n",
        "      M, _ = offsets.shape\n",
        "      proposals = GenerateProposal(pos_anchor_coord.view(1, 1, 1, M, 4), offsets.view(1, 1, 1, M, 4), method='FasterRCNN')\n",
        "      proposals = proposals.view(-1, 4)\n",
        "    if mode == 'train':\n",
        "      return conf_scores, offsets, proposals\n",
        "    elif mode == 'eval':\n",
        "      return conf_scores, offsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNxbLwcKzNR_"
      },
      "source": [
        "def ConfScoreRegression(conf_scores, batch_size):\n",
        "  # the target conf_scores for positive samples are ones and negative are zeros\n",
        "  M = conf_scores.shape[0] // 2\n",
        "  GT_conf_scores = torch.zeros_like(conf_scores)\n",
        "  GT_conf_scores[:M, 0] = 1.\n",
        "  GT_conf_scores[M:, 1] = 1.\n",
        "\n",
        "  conf_score_loss = F.binary_cross_entropy_with_logits(conf_scores, GT_conf_scores, \\\n",
        "                                     reduction='sum') * 1. / batch_size\n",
        "  return conf_score_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvKUmmIpzPEr"
      },
      "source": [
        "def BboxRegression(offsets, GT_offsets, batch_size):\n",
        "  bbox_reg_loss = F.smooth_l1_loss(offsets, GT_offsets, reduction='sum') * 1. / batch_size\n",
        "  return bbox_reg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFHPizwr1jQs"
      },
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "  def __init__(self, reshape_size=224, pooling=False, verbose=False):\n",
        "    super().__init__()\n",
        "\n",
        "    from torchvision import models\n",
        "    from torchsummary import summary\n",
        "\n",
        "    self.mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "    self.mobilenet = nn.Sequential(*list(self.mobilenet.children())[:-1]) # Remove the last classifier\n",
        "\n",
        "    # average pooling\n",
        "    if pooling:\n",
        "      self.mobilenet.add_module('LastAvgPool', nn.AvgPool2d(math.ceil(reshape_size/32.))) # input: N x 1280 x 7 x 7\n",
        "\n",
        "    for i in self.mobilenet.named_parameters():\n",
        "      i[1].requires_grad = True # fine-tune all\n",
        "\n",
        "    if verbose:\n",
        "      summary(self.mobilenet.cuda(), (3, reshape_size, reshape_size))\n",
        "  \n",
        "  def forward(self, img, verbose=False):\n",
        "    num_img = img.shape[0]\n",
        "    \n",
        "    img_prepro = img\n",
        "\n",
        "    feat = []\n",
        "    process_batch = 500\n",
        "    for b in range(math.ceil(num_img/process_batch)):\n",
        "      feat.append(self.mobilenet(img_prepro[b*process_batch:(b+1)*process_batch]\n",
        "                              ).squeeze(-1).squeeze(-1)) # forward and squeeze\n",
        "    feat = torch.cat(feat)\n",
        "    \n",
        "    if verbose:\n",
        "      print('Output feature shape: ', feat.shape)\n",
        "    \n",
        "    return feat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeWgWrzfYgm_"
      },
      "source": [
        "def nms(boxes, scores, iou_threshold=0.5, topk=None):\n",
        "  if (not boxes.numel()) or (not scores.numel()):\n",
        "    return torch.zeros(0, dtype=torch.long)\n",
        "\n",
        "  keep = None\n",
        "  keep = []\n",
        "  x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
        "  areas = (x2 - x1) * (y2 - y1)\n",
        "  sorted, order = torch.sort(scores, descending=True)\n",
        "  while order.shape[0] > 0:   \n",
        "    i = order[0]\n",
        "    keep.append(i)\n",
        "    xx1 = torch.max(x1[i], x1[order[1:]])\n",
        "    yy1 = torch.max(y1[i], y1[order[1:]])\n",
        "    xx2 = torch.min(x2[i], x2[order[1:]])\n",
        "    yy2 = torch.min(y2[i], y2[order[1:]])\n",
        "    intersection = torch.max(torch.zeros(1), xx2 - xx1) * torch.max(torch.zeros(1), yy2 - yy1)\n",
        "    iou = intersection / (areas[i] + areas[order[1:]] - intersection)\n",
        "    index = torch.squeeze(torch.nonzero((iou <= iou_threshold)) + 1)\n",
        "    order = order[index]\n",
        "    order = order.reshape(-1)\n",
        "  if topk is not None:\n",
        "    keep = keep[: topk]\n",
        "  \n",
        "  keep = torch.tensor(keep)\n",
        "  keep.to(boxes.device)\n",
        "  return keep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCKdNkOHzSmr"
      },
      "source": [
        "class RPN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # READ ONLY\n",
        "    self.anchor_list = torch.tensor([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [2, 3], [3, 2], [3, 5], [5, 3]])\n",
        "    self.feat_extractor = FeatureExtractor()\n",
        "    self.prop_module = ProposalModule(1280, num_anchors=self.anchor_list.shape[0])\n",
        "\n",
        "  def forward(self, images, bboxes, output_mode='loss'):\n",
        "    # weights to multiply to each loss term\n",
        "    w_conf = 1 # for conf_scores\n",
        "    w_reg = 5 # for offsets\n",
        "\n",
        "    assert output_mode in ('loss', 'all'), 'invalid output mode!'\n",
        "    total_loss = None\n",
        "    conf_scores, proposals, features, GT_class, pos_anchor_idx, anc_per_img = \\\n",
        "      None, None, None, None, None, None\n",
        "    features = self.feat_extractor(images)\n",
        "    grid_list = GenerateGrid(images.shape[0])\n",
        "    anc_list = GenerateAnchor(self.anchor_list, grid_list).to(bboxes.device, bboxes.dtype)\n",
        "    iou_mat = IoU(anc_list, bboxes)\n",
        "    activated_anc_ind, negative_anc_ind, GT_conf_scores, GT_offsets, GT_class, \\\n",
        "    activated_anc_coord, negative_anc_coord = ReferenceOnActivatedAnchors(anc_list, bboxes, grid_list, iou_mat)\n",
        "    conf_scores, offsets, proposals = self.prop_module(features, activated_anc_coord, activated_anc_ind, negative_anc_ind)\n",
        "    conf_loss = ConfScoreRegression(conf_scores, features.shape[0])\n",
        "    reg_loss = BboxRegression(offsets, GT_offsets, features.shape[0])\n",
        "    anc_per_img = torch.prod(torch.tensor(anc_list.shape[1:-1]))\n",
        "    total_loss = w_conf * conf_loss + w_reg * reg_loss\n",
        "    pos_anchor_idx = activated_anc_ind\n",
        "    if output_mode == 'loss':\n",
        "      return total_loss\n",
        "    else:\n",
        "      return total_loss, conf_scores, proposals, features, GT_class, pos_anchor_idx, anc_per_img\n",
        "\n",
        "\n",
        "  def inference(self, images, thresh=0.5, nms_thresh=0.5, mode='RPN'):\n",
        "    assert mode in ('RPN', 'FasterRCNN'), 'invalid inference mode!'\n",
        "\n",
        "    features, final_conf_scores, final_proposals = None, None, None\n",
        "    final_conf_probs, final_proposals = [], []\n",
        "    features = self.feat_extractor(images)\n",
        "    grid_list = GenerateGrid(images.shape[0])\n",
        "    anc_list = GenerateAnchor(self.anchor_list, grid_list).to(images.device, images.dtype)\n",
        "    conf_scores, offsets = self.prop_module(features)\n",
        "    offsets = offsets.permute(0, 1, 3, 4, 2)\n",
        "    conf_scores = torch.sigmoid(conf_scores)\n",
        "    anchors_proposal = GenerateProposal(anc_list, offsets, method='FasterRCNN')\n",
        "    B, A, _, H, W = conf_scores.shape\n",
        "    for b in range(B):\n",
        "      boxes = []\n",
        "      scores = []\n",
        "      for a in range(A):\n",
        "        for h in range(H):\n",
        "          for w in range(W):\n",
        "            if conf_scores[b, a, 0, h, w] >= thresh:\n",
        "              boxes.append(anchors_proposal[b, a, h, w, :].tolist())\n",
        "              scores.append(conf_scores[b, a, 0, h, w].item())      \n",
        "      boxes = torch.tensor(boxes).to(images.device)\n",
        "      scores = torch.tensor(scores).to(images.device)\n",
        "      import pdb;pdb.set_trace()\n",
        "      # keep = torchvision.ops.nms(boxes, scores, nms_thresh)\n",
        "      keep = nms(boxes, scores, nms_thresh)\n",
        "      final_proposals.append(boxes[keep].reshape(-1, 4))\n",
        "      final_conf_probs.append(scores[keep].reshape(-1, 1))\n",
        "    if mode == 'RPN':\n",
        "      features = [torch.zeros_like(i) for i in final_conf_probs] # dummy class\n",
        "    return final_proposals, final_conf_probs, features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EZFySEgzUoA"
      },
      "source": [
        "def DetectionSolver(detector, train_loader, learning_rate=3e-3,\n",
        "                    lr_decay=1, num_epochs=20, **kwargs):\n",
        "\n",
        "  # ship model to GPU\n",
        "  detector.to(**to_float_cuda)\n",
        "\n",
        "  # optimizer setup\n",
        "  from torch import optim\n",
        "  # optimizer = optim.Adam(\n",
        "  optimizer = optim.SGD(\n",
        "    filter(lambda p: p.requires_grad, detector.parameters()),\n",
        "    learning_rate) # leave betas and eps by default\n",
        "  lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer,\n",
        "                                             lambda epoch: lr_decay ** epoch)\n",
        "\n",
        "  # sample minibatch data\n",
        "  loss_history = []\n",
        "  detector.train()\n",
        "  for i in range(num_epochs):\n",
        "    start_t = time.time()\n",
        "    for iter_num, data_batch in enumerate(train_loader):\n",
        "      images, boxes, w_batch, h_batch, _ = data_batch\n",
        "      resized_boxes = coord_trans(boxes, w_batch, h_batch, mode='p2a')\n",
        "      images = images.to(**to_float_cuda)\n",
        "      resized_boxes = resized_boxes.to(**to_float_cuda)\n",
        "\n",
        "      loss = detector(images, resized_boxes)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      loss_history.append(loss.item())\n",
        "      optimizer.step()\n",
        "\n",
        "      print('(Iter {} / {})'.format(iter_num, len(train_loader)))\n",
        "\n",
        "    end_t = time.time()\n",
        "    print('(Epoch {} / {}) loss: {:.4f} time per epoch: {:.1f}s'.format(\n",
        "        i, num_epochs, loss.item(), end_t-start_t))\n",
        "\n",
        "    lr_scheduler.step()\n",
        "\n",
        "  # plot the training losses\n",
        "  plt.plot(loss_history)\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training loss history')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzdBwHZBzWfG"
      },
      "source": [
        "def DetectionInference(detector, data_loader, dataset, idx_to_class, thresh=0.8, nms_thresh=0.3, output_dir=None):\n",
        "\n",
        "  # ship model to GPU\n",
        "  detector.to(**to_float_cuda)\n",
        " \n",
        "  detector.eval()\n",
        "  start_t = time.time()\n",
        "\n",
        "  if output_dir is not None:\n",
        "    det_dir = 'mAP/input/detection-results'\n",
        "    gt_dir = 'mAP/input/ground-truth'\n",
        "    if os.path.exists(det_dir):\n",
        "      shutil.rmtree(det_dir)\n",
        "    os.mkdir(det_dir)\n",
        "    if os.path.exists(gt_dir):\n",
        "      shutil.rmtree(gt_dir)\n",
        "    os.mkdir(gt_dir)\n",
        "\n",
        "  for iter_num, data_batch in enumerate(data_loader):\n",
        "    # debug: print something\n",
        "    # print(data_batch.shape)\n",
        "\n",
        "    images, boxes, w_batch, h_batch, img_ids = data_batch\n",
        "    images = images.to(**to_float_cuda)\n",
        "\n",
        "    final_proposals, final_conf_scores, final_class = detector.inference(images, thresh=thresh, nms_thresh=nms_thresh)\n",
        "\n",
        "    # clamp on the proposal coordinates\n",
        "    batch_size = len(images)\n",
        "    for idx in range(batch_size):\n",
        "      torch.clamp_(final_proposals[idx][:, 0::2], min=0, max=w_batch[idx])\n",
        "      torch.clamp_(final_proposals[idx][:, 1::2], min=0, max=h_batch[idx])\n",
        "\n",
        "      # visualization\n",
        "      # get the original image\n",
        "      # hack to get the original image so we don't have to load from local again...\n",
        "      i = batch_size*iter_num + idx\n",
        "      img, _ = dataset.__getitem__(i)\n",
        "\n",
        "      valid_box = sum([1 if j != -1 else 0 for j in boxes[idx][:, 0]])\n",
        "      final_all = torch.cat((final_proposals[idx], \\\n",
        "        final_class[idx].float(), final_conf_scores[idx]), dim=-1).cpu()\n",
        "      resized_proposals = coord_trans(final_all, w_batch[idx], h_batch[idx])\n",
        "\n",
        "      # write results to file for evaluation (use mAP API https://github.com/Cartucho/mAP for now...)\n",
        "      if output_dir is not None:\n",
        "        file_name = img_ids[idx].replace('.jpg', '.txt')\n",
        "        with open(os.path.join(det_dir, file_name), 'w') as f_det, \\\n",
        "          open(os.path.join(gt_dir, file_name), 'w') as f_gt:\n",
        "          print('{}: {} GT bboxes and {} proposals'.format(img_ids[idx], valid_box, resized_proposals.shape[0]))\n",
        "          for b in boxes[idx][:valid_box]:\n",
        "            f_gt.write('{} {:.2f} {:.2f} {:.2f} {:.2f}\\n'.format(idx_to_class[b[4].item()], b[0], b[1], b[2], b[3]))\n",
        "          for b in resized_proposals:\n",
        "            f_det.write('{} {:.6f} {:.2f} {:.2f} {:.2f} {:.2f}\\n'.format(idx_to_class[b[4].item()], b[5], b[0], b[1], b[2], b[3]))\n",
        "      else:\n",
        "        data_visualizer(img, idx_to_class, boxes[idx][:valid_box], resized_proposals)\n",
        "\n",
        "  end_t = time.time()\n",
        "  print('Total inference time: {:.1f}s'.format(end_t-start_t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_x3XytKzYQL",
        "outputId": "575a1312-7c8e-414f-80d5-c8d50a02234f"
      },
      "source": [
        "RPNSolver = DetectionSolver # the same solver as in YOLO\n",
        "# monitor the training loss\n",
        "# modified num sample 10 to 5\n",
        "num_sample = 40\n",
        "small_dataset = torch.utils.data.Subset(train_dataset, torch.linspace(0, len(train_dataset)-1, steps=num_sample).long())\n",
        "small_train_loader = pascal_voc2007_loader(small_dataset, 200) # a new loader\n",
        "print(small_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataset.Subset object at 0x7f5d60e98a90>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVQPPDVchAGQ"
      },
      "source": [
        "## RPN - Overfit small data\n",
        "First we will overfit the RPN on a small subset of the PASCAL VOC 2007 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJt9rzAqHAUw"
      },
      "source": [
        "for lr in [1e-3]:\n",
        "  print('lr: ', lr)\n",
        "  rpn = RPN()\n",
        "  RPNSolver(rpn, small_train_loader, learning_rate=lr, num_epochs=600)\n",
        "torch.save(rpn.state_dict(), './gdrive/MyDrive/11785DL/final_project/rpn_small')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOIazbsrFEVc"
      },
      "source": [
        "## RPN - Inference\n",
        "We will now visualize the predicted boxes from the RPN that we overfit to a small training sample. We will reuse the `DetectionInference` function from the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rs3fxgaYzd1u"
      },
      "source": [
        "RPNInference = DetectionInference\n",
        "RPNInference(rpn, small_train_loader, small_dataset, idx_to_class, thresh=0.8, nms_thresh=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umYbQz6Ct2bm"
      },
      "source": [
        "## Faster R-CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXeBUymPJaBX"
      },
      "source": [
        "class TwoStageDetector(nn.Module):\n",
        "  def __init__(self, in_dim=1280, hidden_dim=256, num_classes=20, \\\n",
        "               roi_output_w=2, roi_output_h=2, drop_ratio=0.3):\n",
        "    super().__init__()\n",
        "\n",
        "    assert(num_classes != 0)\n",
        "    self.num_classes = num_classes\n",
        "    self.roi_output_w, self.roi_output_h = roi_output_w, roi_output_h\n",
        "    self.RPN = RPN()\n",
        "    self.region_classification = nn.Sequential(\n",
        "        nn.Linear(in_dim, hidden_dim),\n",
        "        nn.Dropout(p=drop_ratio),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, images, bboxes):\n",
        "    B, _, H, W = images.shape\n",
        "    rpn_loss, conf_scores, proposals, features, GT_class, pos_anchor_idx, anc_per_img = self.RPN(images, bboxes, output_mode='all')\n",
        "    H_prime = features.shape[2]\n",
        "    batch_index = (pos_anchor_idx // anc_per_img).view(-1, 1).to(proposals.dtype)\n",
        "    proposals_index = torch.cat((batch_index, proposals), 1)\n",
        "    roi_feature = torchvision.ops.roi_align(features, proposals_index, (self.roi_output_h, self.roi_output_w))\n",
        "    M = GT_class.shape[0]\n",
        "    mean_pool = torch.nn.AvgPool2d(2)\n",
        "    roi_feature = mean_pool(roi_feature).view(M, -1)\n",
        "    class_probs = self.region_classification(roi_feature)\n",
        "    cls_loss = F.cross_entropy(class_probs, GT_class, reduction='sum') * 1. / M\n",
        "    total_loss = rpn_loss + cls_loss\n",
        "    return total_loss\n",
        "\n",
        "  def inference(self, images, thresh=0.5, nms_thresh=0.7):\n",
        "    final_proposals, final_conf_probs, final_class = None, None, None\n",
        "    final_class = []\n",
        "    B, _, H, W = images.shape\n",
        "    final_proposals, final_conf_probs, features = self.RPN.inference(images, thresh, nms_thresh, mode='FasterRCNN')\n",
        "    for b in range(B):\n",
        "      proposal = final_proposals[b]\n",
        "      if proposal.shape[0] == 0:\n",
        "        final_class.append(torch.tensor([]).to(images.device).reshape(-1, 1))\n",
        "        continue\n",
        "      H_prime = features.shape[2]\n",
        "      index = torch.tensor([b], dtype=images.dtype, device=images.device).view(-1, 1).expand(proposal.shape[0], -1)\n",
        "      proposal_index = torch.cat((index, proposal), 1)\n",
        "      roi_feature = torchvision.ops.roi_align(features, proposal_index, (self.roi_output_h, self.roi_output_w))\n",
        "      mean_pool = torch.nn.AvgPool2d(self.roi_output_h)\n",
        "      roi_feature = mean_pool(roi_feature).view(proposal.shape[0], -1)\n",
        "      class_probs = self.region_classification(roi_feature)\n",
        "      _, class_index = torch.max(class_probs, 1)\n",
        "      final_class.append(class_index.reshape(-1, 1))\n",
        "    return final_proposals, final_conf_probs, final_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFZ49wox4MYn"
      },
      "source": [
        "## Overfit small data\n",
        "We will now overfit the full Faster R-CNN network on a small subset of the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cySKIFSJd-E"
      },
      "source": [
        "lr = 1e-3\n",
        "detector = TwoStageDetector()\n",
        "DetectionSolver(detector, small_train_loader, learning_rate=lr, num_epochs=400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM5KxmaCrnPV"
      },
      "source": [
        "torch.save(detector.state_dict(), './gdrive/MyDrive/11785DL/final_project/detector')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G8bU4zRJesl"
      },
      "source": [
        "DetectionInference(detector, small_train_loader, small_dataset, idx_to_class, thresh=0.7, nms_thresh=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr7wNngy4oZf"
      },
      "source": [
        "## Train a net\n",
        "Now it's time to train the full Faster R-CNN model on a larger subset of the the training data. We will train for 50 epochs;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRHzAE6OJgnW"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "train_loader = pascal_voc2007_loader(train_dataset, 100) # a new loader\n",
        "\n",
        "num_epochs = 50\n",
        "lr = 5e-3\n",
        "frcnn_detector = TwoStageDetector()\n",
        "DetectionSolver(frcnn_detector, train_loader, learning_rate=lr, num_epochs=num_epochs)\n",
        "model_save_name = 'frcnn_detector.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "torch.save(frcnn_detector.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O_umtnKvJi3V"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "frcnn_detector = TwoStageDetector()\n",
        "model_save_name = 'frcnn_detector.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "frcnn_detector.load_state_dict(torch.load(path))\n",
        "DetectionInference(frcnn_detector, small_train_loader, small_dataset, idx_to_class, thresh=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG57dJb7JlPD"
      },
      "source": [
        "!rm -r mAP/input/*\n",
        "# DetectionInference(frcnn_detector, val_loader, val_dataset, idx_to_class, output_dir='mAP/input', thresh=0.8, nms_thresh=0.3)\n",
        "DetectionInference(frcnn_detector, train_loader, train_dataset, idx_to_class, output_dir='mAP/input', thresh=0.8, nms_thresh=0.3) \n",
        "!cd mAP && python main.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNNixFs-Ylwg"
      },
      "source": [
        "!tar -xvf gdrive/MyDrive/11785DL/final_project/VOCtrainval_06-Nov-2007.tar"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}